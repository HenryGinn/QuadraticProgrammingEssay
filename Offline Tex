% Explanation on special cases

$w_j + \alpha_j u + \sum_{i=1}^{n-1} \alpha_{ij}u_i = b_j$

$u = \frac{1}{\alpha_j} \left( b_j - w_j - \sum_{i=1}^{n-1} \alpha_{ij}u_i \right)$

$w_k + \frac{\alpha_k}{\alpha_j}\left( b_j - w_j - \sum_{i=1}^{n-1} \alpha_{ij}u_i \right) + \sum_{i=1}^{n-1} \alpha_{ik}u_i = b_k$

$w_k - \frac{\alpha_k}{\alpha_j}w_j - \sum_{i=1}^{n-1} \alpha'_{ik}u_i = b_k - \frac{\alpha_k}{\alpha_j}b_j$


% Special cases to consider:

% \alpha_j = 0
% If \alpha_j = 0 then the normal vector of the constraint corresponding to w_j is perpendicular to the normal vector corresponding to u (the entering variable). This means the line of intersection of the other basic variables is either parallel to the w_j constraint (b \ne 0) or it lies within the w_j constraint (b = 0). We are letting u enter the set of basic variables, but we are not removing anything from the set of basic variables. This means there are n-1 non-basic variables, and the intersection of the n-1 linearly independent constraints corresponding to those non-basic variables define a line.

Pivot row: $w_j + \sum_{i=1}^{n-1} \alpha_{ij}u_i = b_j$
Arbitrary row: $w_k + \alpha_k u + \sum_{i=1}^{n-1} \alpha_{ik}u_i = b_k$

Equations:
$0 = b_j - w_j - \sum_{i=1}^{n-1} \alpha_{ij}u_i$
$w_k = b_k - \alpha_k u - \sum_{i=1}^{n-1} \alpha_{ik}u_i$
As usual we set our non-basic variables equal to 0 and this gives us

$0 = b_j - w_j$
$w_k = b_k - \alpha_k u$

% The first equation fixes the value of w_j. This equation should be fixing the value of u, but as we could not make w_j an exiting variable, it's value is fixed and u is a free variable. If we treat u as a parameter then the other n-1 basic variables become fixed and we have defined a unique point on the line for a given value of u. This line points in the direction of the normal vector for the constraint associated with u. The value of w_j, b_j, gives the minimum distance from the line being traversed and the constraint associated with w_j. We cannot choose u < 0 as we will break the constraint associated with u. We need all the w_k to remain non-negative, so we need u \le \frac{b_k}{\alpha_k} for all k where this fraction is positive, which is equivalent to u \le min_k \frac{b_k}{\alpha_k} (where the fraction is positive). This corresponds to hitting another constraint. We see that this is precisely the constraint we would have hit if we had ignored this row, even if b_j = 0. For b \ne 0, we can gain some intuition for this by considering what happens as \alpha_j \to 0. As \alpha_j \to 0_-, w_j could not be our exiting variable as we would be moving into the infeasible region. As \alpha_j \to 0_+, we see that \frac{\b_j}{\alpha_j} \to \infty, which corresponds to the intersection point moving further and further away. 


% b_j = 0, \alpha_j \ne 0
% We will be able to eliminate u_j from the other rows, but the profit will not change. This is the case where we have moved to a constraint that intersects the vertex we are already at, and so we do not move at all. Where b_j > 0 we rejected the case where \alpha_j < 0 as it would move us into the infeasible region, but when b_j = 0 we do not move at all so we remain feasible. 


% All \alpha_k \le 0
% If this is the case then we cannot choose an exiting variable. As we move away from the constraint associated with u, we do not break any constraints. In fact, the only constraints we could hit are those where \alpha_j = 0 with b_j = 0, and here the line on which we are travelling on lies within those constraints and so the solution is unbounded.




Once the slack variables are added we have a system of equations, ${Ax = b}$. We can write $x_B$ and $x_N$ have components equal to the basic and non-basic variables respectively. Similarly we define the matrices $A_B$ and $A_N$ which have the columns of the indices of the basic and non-basic variables respectively. In these new variables the system ${Ax = b}$ turns into ${A_B x_B + A_N x_N = b}$, and similarly the objective function can be written as ${c_B^T x_B + c_N^T x_N}$.

This can be rearranged as the following:
\begin{align*}
    x_B &= A_B^{-1} b - A_B^{-1} A_N x_N & z &= c_B^T A_B^{-1} b + (c_N^T - c_B^T A_B^{-1} A_N) x_N
\end{align*}

These describe the relevant parts of the tableau and the profit row respectively.

We never find the inverse of $A_B$. Instead we compute it's LU factorisation and use it to solve linear systems. This is because it is faster, saves memory, and is more stable.

The tableau is ${x_B = A_B^{-1} b - A_B^{-1} A_N x_N}$. The components of $x_N$ are 0, so the first term here , $A_B^{-1} b$, gives the value of $x_B$ (the numbers in the value column of the tableau). The second term forms the non-basic part of the tableau, and we use this to find the pivot column. We write $A_i$ for the column of $A_N$ corresponding to the i'th non-basic variable, and ${A_B^{-1} A_i}$ gives the value of the column in the tableau. We can find this by solving the linear system ${A_B v = A_i}$ for $v$.

Similarly we can extract information from the profit row. The profit is given by ${c_B^T A_B^{-1} b}$, and we note that this is the same as finding the dot product of $c_B$ with the values of the basic variables found above. ${c_N^T - c_B^T A_B^{-1} A_N}$ gives the non-basic values in the profit row. Taking the transpose of this gives ${c_N - A_N^T A_B^{-T} c_B}$. We can find the value of ${A_B^{-T} c_B}$ by solving the linear system ${A_B^T v = c_B}$, and then substitute this in to find the profit row. We note that once the LU factorisation of $A$ has been found, the LU factorisation of $A^T$ is also given as ${A^T = (LU)^T = U^T L^T}$ and $U^T$, $L^T$ are lower and upper triangular matrices respectively.


